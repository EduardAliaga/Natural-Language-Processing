{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from types import SimpleNamespace\nfrom collections import Counter\nimport os\nimport re\nimport pathlib\nimport array\nimport pickle\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport pandas as pd","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":1.589337,"end_time":"2022-03-07T15:19:34.024637","exception":false,"start_time":"2022-03-07T15:19:32.4353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.570942Z","iopub.execute_input":"2023-03-08T07:23:30.571408Z","iopub.status.idle":"2023-03-08T07:23:30.577477Z","shell.execute_reply.started":"2023-03-08T07:23:30.571367Z","shell.execute_reply":"2023-03-08T07:23:30.576237Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import time\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nfrom typing import Tuple, Dict, Any, List\nfrom torchvision import datasets, transforms\n\nimport matplotlib\n%matplotlib inline\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2023-03-08T07:23:30.579456Z","iopub.execute_input":"2023-03-08T07:23:30.580114Z","iopub.status.idle":"2023-03-08T07:23:30.593382Z","shell.execute_reply.started":"2023-03-08T07:23:30.580078Z","shell.execute_reply":"2023-03-08T07:23:30.592294Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"DATASET_VERSION = 'ca-100'\nCOMPETITION_ROOT = '../input/vectors4'\nDATASET_ROOT = f'../input/text-preprocessing/data/{DATASET_VERSION}'\nWORKING_ROOT = f'data/{DATASET_VERSION}'\nDATASET_PREFIX = 'ca.wiki'","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","papermill":{"duration":0.033113,"end_time":"2022-03-07T15:19:34.086043","exception":false,"start_time":"2022-03-07T15:19:34.05293","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.594734Z","iopub.execute_input":"2023-03-08T07:23:30.595585Z","iopub.status.idle":"2023-03-08T07:23:30.608900Z","shell.execute_reply.started":"2023-03-08T07:23:30.595550Z","shell.execute_reply":"2023-03-08T07:23:30.608058Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"params = SimpleNamespace(\n    embedding_dim = 400,\n    window_size = 7,\n    batch_size = 1000,\n    epochs = 4,\n    preprocessed = f'{DATASET_ROOT}/{DATASET_PREFIX}',\n    working = f'{WORKING_ROOT}/{DATASET_PREFIX}',\n    modelname = f'{WORKING_ROOT}/{DATASET_VERSION}.pt',\n    train = True\n)","metadata":{"papermill":{"duration":0.022327,"end_time":"2022-03-07T15:19:34.123904","exception":false,"start_time":"2022-03-07T15:19:34.101577","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.612948Z","iopub.execute_input":"2023-03-08T07:23:30.613259Z","iopub.status.idle":"2023-03-08T07:23:30.619883Z","shell.execute_reply.started":"2023-03-08T07:23:30.613235Z","shell.execute_reply":"2023-03-08T07:23:30.618890Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"class Vocabulary(object):\n    def __init__(self, pad_token='<pad>', unk_token='<unk>', eos_token='<eos>'):\n        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token\n        if pad_token is not None:\n            self.pad_index = self.add_token(pad_token)\n        if unk_token is not None:\n            self.unk_index = self.add_token(unk_token)\n        if eos_token is not None:\n            self.eos_index = self.add_token(eos_token)\n\n    def add_token(self, token):\n        if token not in self.token2idx:\n            self.idx2token.append(token)\n            self.token2idx[token] = len(self.idx2token) - 1\n        return self.token2idx[token]\n\n    def get_index(self, token):\n        if isinstance(token, str):\n            return self.token2idx.get(token, self.unk_index)\n        else:\n            return [self.token2idx.get(t, self.unk_index) for t in token]\n\n    def get_token(self, index):\n        return self.idx2token[index]\n\n    def __len__(self):\n        return len(self.idx2token)\n\n    def save(self, filename):\n        with open(filename, 'wb') as f:\n            pickle.dump(self.__dict__, f)\n\n    def load(self, filename):\n        with open(filename, 'rb') as f:\n            self.__dict__.update(pickle.load(f))","metadata":{"papermill":{"duration":0.027867,"end_time":"2022-03-07T15:19:34.166984","exception":false,"start_time":"2022-03-07T15:19:34.139117","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.621446Z","iopub.execute_input":"2023-03-08T07:23:30.622097Z","iopub.status.idle":"2023-03-08T07:23:30.633228Z","shell.execute_reply.started":"2023-03-08T07:23:30.622062Z","shell.execute_reply":"2023-03-08T07:23:30.632291Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"class SkipGram(nn.Module):\n    def _init_(self, vocab_size, embed_size):\n        super(SkipGram, self)._init_()\n        self.embeddings = nn.Embedding(vocab_size, embed_size)\n        self.linear = nn.Linear(embed_size, vocab_size, bias = False)\n    \n    def forward(self, inputs):\n        embeds = self.embeddings(inputs)  \n        embeds = embeds.unsqueeze(1).repeat(1, 6, 1)\n        output = self.linear(embeds)  \n        return output","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def batch_generator(idata, target, batch_size, shuffle=True):\n    nsamples = len(idata)\n    if shuffle:\n        perm = np.random.permutation(nsamples)\n    else:\n        perm = range(nsamples)\n\n    for i in range(0, nsamples, batch_size):\n        batch_idx = perm[i:i+batch_size]\n        if target is not None:\n            yield idata[batch_idx], target[batch_idx]\n        else:\n            yield idata[batch_idx], None","metadata":{"papermill":{"duration":0.022805,"end_time":"2022-03-07T15:19:34.204737","exception":false,"start_time":"2022-03-07T15:19:34.181932","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.636767Z","iopub.execute_input":"2023-03-08T07:23:30.637190Z","iopub.status.idle":"2023-03-08T07:23:30.647165Z","shell.execute_reply.started":"2023-03-08T07:23:30.637165Z","shell.execute_reply":"2023-03-08T07:23:30.646278Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"CBOW model\n----------\nYou can add new parameters to the model in the *\\_\\_init\\_\\_()* method with *self.register_buffer()* (for parameters not to be trained):\n\n    self.register_buffer('position_weight', torch.tensor([1,2,3,3,2,1], dtype=torch.float32))\n\nor *nn.Parameter()* (for parameters to be trained)\n\n    self.position_weight = nn.Parameter(torch.tensor([1,2,3,3,2,1], dtype=torch.float32))\n    \nIn both cases, you can reference and use them in the *forward* method as\n\n    self.position_weight","metadata":{"papermill":{"duration":0.014925,"end_time":"2022-03-07T15:19:34.234589","exception":false,"start_time":"2022-03-07T15:19:34.219664","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#ejercicio 1\nclass CBOW(nn.Module):\n    def __init__(self, num_embeddings, embedding_dim):\n        super().__init__()\n        self.emb = nn.Embedding(num_embeddings, embedding_dim, padding_idx=0)\n        self.lin = nn.Linear(embedding_dim, num_embeddings, bias=False)\n        #Con un escalar fijo:\n        #self.register_buffer('position_weight', torch.tensor([1,2,3,3,2,1], dtype=torch.float32))\n        #.view(1,-1,1)\n        #Con un escalar entrenado\n        #self.position_weight = nn.Parameter(torch.tensor([1,2,3,3,2,1], dtype=torch.float32))\n        #Con un vector entrenado:\n        self.position_weight = nn.Parameter(torch.randn(6,embedding_dim), requires_grad=True)\n        \n\n    # B = Batch size\n    # W = Number of context words (left + right)\n    # E = embedding_dim\n    # V = num_embeddings (number of words)\n    def forward(self, input):\n        # input shape is (B, W)\n        e = self.emb(input)\n        weighted_e=e*self.position_weight\n        # e shape is (B, W, E)\n        u = weighted_e.sum(dim=1)\n        # u shape is (B, E)\n        z = self.lin(u)\n        # z shape is (B, V)\n        return z","metadata":{"papermill":{"duration":0.023523,"end_time":"2022-03-07T15:19:34.275697","exception":false,"start_time":"2022-03-07T15:19:34.252174","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.648816Z","iopub.execute_input":"2023-03-08T07:23:30.649219Z","iopub.status.idle":"2023-03-08T07:23:30.658204Z","shell.execute_reply.started":"2023-03-08T07:23:30.649140Z","shell.execute_reply":"2023-03-08T07:23:30.657279Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"We can visually show how this model works:\n\n<div style=\"text-align:center\">\n    <img src=\"https://drive.google.com/uc?id=1INeftgKRCS0QWZu9InCDZ42a5fph3jQX\" alt=\"drawing\" width=\"800\" height=\"800\"/>\n</div>\n\nWe train the model to learn matrices $\\mathbf{E}$ and $\\mathbf{W}$. Probabilities $P(w_t|w_{c-m},\\cdots,w_{c+m})$ can be obtained by applying the softmax function to the output vector $\\mathbf{z}$. However, our loss function [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) takes care of this.\n","metadata":{"papermill":{"duration":0.015071,"end_time":"2022-03-07T15:19:34.306676","exception":false,"start_time":"2022-03-07T15:19:34.291605","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_preprocessed_dataset(prefix):\n    # Try loading precomputed vocabulary and preprocessed data files\n    token_vocab = Vocabulary()\n    token_vocab.load(f'{prefix}.vocab')\n    data = []\n    for part in ['train', 'valid', 'test']:\n        with np.load(f'{prefix}.{part}.npz') as set_data:\n            idata, target = set_data['idata'], set_data['target']\n            data.append((idata, target))\n            print(f'Number of samples ({part}): {len(target)}')\n    print(\"Using precomputed vocabulary and data files\")\n    print(f'Vocabulary size: {len(token_vocab)}')\n    return token_vocab, data","metadata":{"papermill":{"duration":0.023472,"end_time":"2022-03-07T15:19:34.345176","exception":false,"start_time":"2022-03-07T15:19:34.321704","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.659759Z","iopub.execute_input":"2023-03-08T07:23:30.660473Z","iopub.status.idle":"2023-03-08T07:23:30.670624Z","shell.execute_reply.started":"2023-03-08T07:23:30.660437Z","shell.execute_reply":"2023-03-08T07:23:30.669686Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"def load_preprocessed_dataset_SkipGram(prefix):\n    # Try loading precomputed vocabulary and preprocessed data files\n    token_vocab = Vocabulary()\n    token_vocab.load(f'{prefix}.vocab')\n    data = []\n    for part in ['train', 'valid', 'test']:\n        with np.load(f'{prefix}.{part}.npz') as set_data:\n            target, idata = set_data['idata'], set_data['target']\n            data.append((idata, target))\n            print(f'Number of samples ({part}): {len(target)}')\n    print(\"Using precomputed vocabulary and data files\")\n    print(f'Vocabulary size: {len(token_vocab)}')\n    return token_vocab, data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(model, criterion, optimizer,scheduler, idata, target, batch_size, device, log=False):\n    model.train()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    for X, y in batch_generator(idata, target, batch_size, shuffle=True):\n        # Get input and target sequences from batch\n        X = torch.tensor(X, dtype=torch.long, device=device)\n        y = torch.tensor(y, dtype=torch.long, device=device)\n\n        model.zero_grad()\n        output = model(X)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        # Training statistics\n        total_loss += loss.item()\n        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n        ntokens += y.numel()\n        niterations += 1\n        if niterations == 200 or niterations == 500 or niterations % 1000 == 0:\n            print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={100*ncorrect/ntokens:.1f}, loss={total_loss/ntokens:.2f}')\n\n    total_loss = total_loss / ntokens\n    accuracy = 100 * ncorrect / ntokens\n    if log:\n        print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={accuracy:.1f}, loss={total_loss:.2f}')\n    return accuracy, total_loss","metadata":{"papermill":{"duration":0.026206,"end_time":"2022-03-07T15:19:34.38637","exception":false,"start_time":"2022-03-07T15:19:34.360164","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.705503Z","iopub.execute_input":"2023-03-08T07:23:30.706186Z","iopub.status.idle":"2023-03-08T07:23:30.714999Z","shell.execute_reply.started":"2023-03-08T07:23:30.706149Z","shell.execute_reply":"2023-03-08T07:23:30.714187Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def train_SkipGram(model, criterion, optimizer, idata, target, batch_size, device, log=False):\n    model.train()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    \n\n    for X, y in batch_generator(idata, target, batch_size, shuffle=True):\n        # Get input and target sequences from batch\n        X = torch.tensor(X, dtype=torch.long, device=device)\n        y = torch.tensor(y, dtype=torch.long, device=device)\n        \n        model.zero_grad()\n        output = model(X)\n        \n        output = output.permute(0, 2, 1)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        # Training statistics\n        total_loss += loss.item()\n        ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n        ntokens += y.numel()\n        niterations += 1\n        if niterations == 200 or niterations == 500 or niterations % 1000 == 0:\n            print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={100*ncorrect/ntokens:.1f}, loss={total_loss/ntokens:.2f}')\n\n    total_loss = total_loss / ntokens\n    accuracy = 100 * ncorrect / ntokens\n    if log:\n        print(f'Train: wpb={ntokens//niterations}, num_updates={niterations}, accuracy={accuracy:.1f}, loss={total_loss:.2f}')\n    return accuracy, total_loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def validate(model, criterion, idata, target, batch_size, device):\n    model.eval()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    y_pred = []\n    with torch.no_grad():\n        for X, y in batch_generator(idata, target, batch_size, shuffle=False):\n            # Get input and target sequences from batch\n            X = torch.tensor(X, dtype=torch.long, device=device)\n            output = model(X)\n            if target is not None:\n                y = torch.tensor(y, dtype=torch.long, device=device)\n                loss = criterion(output, y)\n                total_loss += loss.item()\n                ncorrect += (torch.max(output, 1)[1] == y).sum().item()\n                ntokens += y.numel()\n                niterations += 1\n            else:\n                pred = torch.max(output, 1)[1].detach().to('cpu').numpy()\n                y_pred.append(pred)\n\n    if target is not None:\n        total_loss = total_loss / ntokens\n        accuracy = 100 * ncorrect / ntokens\n        return accuracy, total_loss\n    else:\n        return np.concatenate(y_pred)","metadata":{"papermill":{"duration":0.025933,"end_time":"2022-03-07T15:19:34.428633","exception":false,"start_time":"2022-03-07T15:19:34.4027","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.717533Z","iopub.execute_input":"2023-03-08T07:23:30.717996Z","iopub.status.idle":"2023-03-08T07:23:30.731685Z","shell.execute_reply.started":"2023-03-08T07:23:30.717942Z","shell.execute_reply":"2023-03-08T07:23:30.730725Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"def validate_SkipGram(model, criterion, idata, target, batch_size, device):\n    model.eval()\n    total_loss = 0\n    ncorrect = 0\n    ntokens = 0\n    niterations = 0\n    y_pred = []\n    target = torch.from_numpy(target)\n    target = target.float()\n    with torch.no_grad():\n        for X, y in batch_generator(idata, target, batch_size, shuffle=False):\n            # Get input and target sequences from batch\n            X = torch.tensor(X, dtype=torch.long, device=device)\n            output = model(X)\n             output = output.permute(0, 2, 1)\n            if target is not None:\n                y = torch.tensor(y, dtype=torch.long, device=device)\n                loss = criterion(output, y)\n                total_loss += loss.item()\n                ncorrect += (output == y).sum().item()\n                ntokens += y.numel()\n                niterations += 1\n            else:\n                pred = torch.max(output, 1)[1].detach().to('cpu').numpy()\n                y_pred.append(pred)\n\n    if target is not None:\n        total_loss = total_loss / ntokens\n        accuracy = 100 * ncorrect / ntokens\n        return accuracy, total_loss\n    else:\n        return np.concatenate(y_pred)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create working dir\npathlib.Path(WORKING_ROOT).mkdir(parents=True, exist_ok=True)","metadata":{"papermill":{"duration":0.021622,"end_time":"2022-03-07T15:19:34.465396","exception":false,"start_time":"2022-03-07T15:19:34.443774","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.733325Z","iopub.execute_input":"2023-03-08T07:23:30.733958Z","iopub.status.idle":"2023-03-08T07:23:30.744308Z","shell.execute_reply.started":"2023-03-08T07:23:30.733925Z","shell.execute_reply":"2023-03-08T07:23:30.743116Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Select device\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    print(\"WARNING: Training without GPU can be very slow!\")","metadata":{"papermill":{"duration":0.070662,"end_time":"2022-03-07T15:19:34.551015","exception":false,"start_time":"2022-03-07T15:19:34.480353","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.745673Z","iopub.execute_input":"2023-03-08T07:23:30.746135Z","iopub.status.idle":"2023-03-08T07:23:30.754942Z","shell.execute_reply.started":"2023-03-08T07:23:30.746093Z","shell.execute_reply":"2023-03-08T07:23:30.753974Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"vocab, data = load_preprocessed_dataset(params.preprocessed)","metadata":{"papermill":{"duration":18.677691,"end_time":"2022-03-07T15:19:53.244311","exception":false,"start_time":"2022-03-07T15:19:34.56662","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:30.758369Z","iopub.execute_input":"2023-03-08T07:23:30.758787Z","iopub.status.idle":"2023-03-08T07:23:33.830136Z","shell.execute_reply.started":"2023-03-08T07:23:30.758762Z","shell.execute_reply":"2023-03-08T07:23:33.829110Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Number of samples (train): 82284341\nNumber of samples (valid): 164765\nNumber of samples (test): 165837\nUsing precomputed vocabulary and data files\nVocabulary size: 100002\n","output_type":"stream"}]},{"cell_type":"code","source":"\"\"\"        self.token2idx = {}\n        self.idx2token = []\n        self.pad_token = pad_token\n        self.unk_token = unk_token\n        self.eos_token = eos_token \"\"\"\nprint(vocab.eos_token)","metadata":{"execution":{"iopub.status.busy":"2023-03-08T07:23:33.831505Z","iopub.execute_input":"2023-03-08T07:23:33.832138Z","iopub.status.idle":"2023-03-08T07:23:33.839059Z","shell.execute_reply.started":"2023-03-08T07:23:33.832100Z","shell.execute_reply":"2023-03-08T07:23:33.837184Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"# 'El Periodico' validation dataset\nvalid_x_df = pd.read_csv(f'{COMPETITION_ROOT}/x_valid.csv')\ntokens = valid_x_df.columns[1:]\nvalid_x = valid_x_df[tokens].apply(vocab.get_index).to_numpy(dtype='int32')\nvalid_y_df = pd.read_csv(f'{COMPETITION_ROOT}/y_valid.csv')\nvalid_y = valid_y_df['token'].apply(vocab.get_index).to_numpy(dtype='int32')","metadata":{"papermill":{"duration":0.212202,"end_time":"2022-03-07T15:19:53.472681","exception":false,"start_time":"2022-03-07T15:19:53.260479","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:33.840332Z","iopub.execute_input":"2023-03-08T07:23:33.841198Z","iopub.status.idle":"2023-03-08T07:23:33.970303Z","shell.execute_reply.started":"2023-03-08T07:23:33.841159Z","shell.execute_reply":"2023-03-08T07:23:33.969352Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# 'El Periodico' test dataset\nvalid_x_df = pd.read_csv(f'{COMPETITION_ROOT}/x_test.csv')\ntest_x = valid_x_df[tokens].apply(vocab.get_index).to_numpy(dtype='int32')","metadata":{"papermill":{"duration":0.156664,"end_time":"2022-03-07T15:19:53.646065","exception":false,"start_time":"2022-03-07T15:19:53.489401","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:33.973005Z","iopub.execute_input":"2023-03-08T07:23:33.973696Z","iopub.status.idle":"2023-03-08T07:23:34.078462Z","shell.execute_reply.started":"2023-03-08T07:23:33.973658Z","shell.execute_reply":"2023-03-08T07:23:34.077442Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"model = CBOW(len(vocab), params.embedding_dim).to(device)\nprint(model)\nfor name, param in model.named_parameters():\n    print(f'{name:20} {param.numel()} {list(param.shape)}')\nprint(f'TOTAL                {sum(p.numel() for p in model.parameters())}')\n","metadata":{"papermill":{"duration":3.281451,"end_time":"2022-03-07T15:19:56.94324","exception":false,"start_time":"2022-03-07T15:19:53.661789","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:34.080294Z","iopub.execute_input":"2023-03-08T07:23:34.080681Z","iopub.status.idle":"2023-03-08T07:23:34.881344Z","shell.execute_reply.started":"2023-03-08T07:23:34.080642Z","shell.execute_reply":"2023-03-08T07:23:34.880301Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"CBOW(\n  (emb): Embedding(100002, 400, padding_idx=0)\n  (lin): Linear(in_features=400, out_features=100002, bias=False)\n)\nposition_weight      2400 [6, 400]\nemb.weight           40000800 [100002, 400]\nlin.weight           40000800 [100002, 400]\nTOTAL                80004000\n","output_type":"stream"}]},{"cell_type":"code","source":"model = SkipGram(len(vocab), params.embedding_dim).to(device)\nprint(model)\nfor name, param in model.named_parameters():\n    print(f'{name:20} {param.numel()} {list(param.shape)}')\nprint(f'TOTAL                {sum(p.numel() for p in model.parameters())}')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Note that $\\textit{lin}$ layer transposes $\\textit{lin.weight}$ before performing the multilplication.","metadata":{"papermill":{"duration":0.016044,"end_time":"2022-03-07T15:19:56.97573","exception":false,"start_time":"2022-03-07T15:19:56.959686","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"The [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) criterion combines *nn.LogSoftmax()* and *nn.NLLLoss()* in one single class.","metadata":{"papermill":{"duration":0.015758,"end_time":"2022-03-07T15:19:57.007432","exception":false,"start_time":"2022-03-07T15:19:56.991674","status":"completed"},"tags":[]}},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss(reduction='sum') #ya fa la softmax","metadata":{"papermill":{"duration":0.024301,"end_time":"2022-03-07T15:19:57.04737","exception":false,"start_time":"2022-03-07T15:19:57.023069","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:34.882815Z","iopub.execute_input":"2023-03-08T07:23:34.883443Z","iopub.status.idle":"2023-03-08T07:23:34.889223Z","shell.execute_reply.started":"2023-03-08T07:23:34.883405Z","shell.execute_reply":"2023-03-08T07:23:34.888295Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from torch.optim.lr_scheduler import OneCycleLR\noptimizer = torch.optim.Adam(model.parameters())\nscheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(vocab),\n                       epochs=params.epochs, anneal_strategy='cos', cycle_momentum=False)\ntrain_accuracy = []\nwiki_accuracy = []\nvalid_accuracy = []\nfor epoch in range(params.epochs):\n    acc, loss = train(model, criterion, optimizer,scheduler, data[0][0], data[0][1], params.batch_size, device, log=True)\n    train_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | train accuracy={acc:.1f}%, train loss={loss:.2f}')\n    acc, loss = validate(model, criterion, data[1][0], data[1][1], params.batch_size, device)\n    wiki_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%, valid loss={loss:.2f} (wikipedia)')\n    acc, loss = validate(model, criterion, valid_x, valid_y, params.batch_size, device)\n    valid_accuracy.append(acc)\n    print(f'| epoch {epoch:03d} | valid accuracy={acc:.1f}%, valid loss={loss:.2f} (El Periódico)')\n\n\n# Save model\ntorch.save(model.state_dict(), params.modelname)","metadata":{"papermill":{"duration":7964.646029,"end_time":"2022-03-07T17:32:41.708969","exception":false,"start_time":"2022-03-07T15:19:57.06294","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-03-08T07:23:34.890515Z","iopub.execute_input":"2023-03-08T07:23:34.891084Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Train: wpb=1000, num_updates=200, accuracy=6.2, loss=9.84\nTrain: wpb=1000, num_updates=500, accuracy=11.4, loss=8.45\nTrain: wpb=1000, num_updates=1000, accuracy=15.0, loss=7.57\nTrain: wpb=1000, num_updates=2000, accuracy=18.5, loss=6.82\nTrain: wpb=1000, num_updates=3000, accuracy=20.6, loss=6.43\nTrain: wpb=1000, num_updates=4000, accuracy=22.2, loss=6.16\nTrain: wpb=1000, num_updates=5000, accuracy=23.4, loss=5.96\nTrain: wpb=1000, num_updates=6000, accuracy=24.5, loss=5.80\nTrain: wpb=1000, num_updates=7000, accuracy=25.4, loss=5.67\nTrain: wpb=1000, num_updates=8000, accuracy=26.2, loss=5.56\nTrain: wpb=1000, num_updates=9000, accuracy=27.0, loss=5.46\nTrain: wpb=1000, num_updates=10000, accuracy=27.6, loss=5.38\nTrain: wpb=1000, num_updates=11000, accuracy=28.2, loss=5.30\nTrain: wpb=1000, num_updates=12000, accuracy=28.7, loss=5.23\nTrain: wpb=1000, num_updates=13000, accuracy=29.2, loss=5.17\nTrain: wpb=1000, num_updates=14000, accuracy=29.7, loss=5.12\nTrain: wpb=1000, num_updates=15000, accuracy=30.1, loss=5.06\nTrain: wpb=1000, num_updates=16000, accuracy=30.5, loss=5.01\nTrain: wpb=1000, num_updates=17000, accuracy=30.9, loss=4.97\nTrain: wpb=1000, num_updates=18000, accuracy=31.3, loss=4.93\nTrain: wpb=1000, num_updates=19000, accuracy=31.6, loss=4.89\nTrain: wpb=1000, num_updates=20000, accuracy=32.0, loss=4.85\nTrain: wpb=1000, num_updates=21000, accuracy=32.3, loss=4.81\nTrain: wpb=1000, num_updates=22000, accuracy=32.6, loss=4.78\nTrain: wpb=1000, num_updates=23000, accuracy=32.8, loss=4.75\nTrain: wpb=1000, num_updates=24000, accuracy=33.1, loss=4.72\nTrain: wpb=1000, num_updates=25000, accuracy=33.4, loss=4.69\nTrain: wpb=1000, num_updates=26000, accuracy=33.6, loss=4.66\nTrain: wpb=1000, num_updates=27000, accuracy=33.8, loss=4.63\nTrain: wpb=1000, num_updates=28000, accuracy=34.1, loss=4.61\nTrain: wpb=1000, num_updates=29000, accuracy=34.3, loss=4.59\nTrain: wpb=1000, num_updates=30000, accuracy=34.5, loss=4.56\nTrain: wpb=1000, num_updates=31000, accuracy=34.7, loss=4.54\nTrain: wpb=1000, num_updates=32000, accuracy=34.9, loss=4.52\nTrain: wpb=1000, num_updates=33000, accuracy=35.1, loss=4.50\nTrain: wpb=1000, num_updates=34000, accuracy=35.3, loss=4.47\nTrain: wpb=1000, num_updates=35000, accuracy=35.4, loss=4.45\nTrain: wpb=1000, num_updates=36000, accuracy=35.6, loss=4.44\nTrain: wpb=1000, num_updates=37000, accuracy=35.8, loss=4.42\nTrain: wpb=1000, num_updates=38000, accuracy=35.9, loss=4.40\nTrain: wpb=1000, num_updates=39000, accuracy=36.1, loss=4.38\nTrain: wpb=1000, num_updates=40000, accuracy=36.2, loss=4.36\nTrain: wpb=1000, num_updates=41000, accuracy=36.4, loss=4.35\nTrain: wpb=1000, num_updates=42000, accuracy=36.5, loss=4.33\nTrain: wpb=1000, num_updates=43000, accuracy=36.6, loss=4.32\nTrain: wpb=1000, num_updates=44000, accuracy=36.8, loss=4.30\nTrain: wpb=1000, num_updates=45000, accuracy=36.9, loss=4.29\nTrain: wpb=1000, num_updates=46000, accuracy=37.0, loss=4.27\nTrain: wpb=1000, num_updates=47000, accuracy=37.1, loss=4.26\nTrain: wpb=1000, num_updates=48000, accuracy=37.3, loss=4.25\nTrain: wpb=1000, num_updates=49000, accuracy=37.4, loss=4.23\nTrain: wpb=1000, num_updates=50000, accuracy=37.5, loss=4.22\nTrain: wpb=1000, num_updates=51000, accuracy=37.6, loss=4.21\nTrain: wpb=1000, num_updates=52000, accuracy=37.7, loss=4.19\nTrain: wpb=1000, num_updates=53000, accuracy=37.8, loss=4.18\nTrain: wpb=1000, num_updates=54000, accuracy=37.9, loss=4.17\nTrain: wpb=1000, num_updates=55000, accuracy=38.0, loss=4.16\nTrain: wpb=1000, num_updates=56000, accuracy=38.1, loss=4.15\nTrain: wpb=1000, num_updates=57000, accuracy=38.1, loss=4.14\nTrain: wpb=1000, num_updates=58000, accuracy=38.2, loss=4.13\nTrain: wpb=1000, num_updates=59000, accuracy=38.3, loss=4.12\nTrain: wpb=1000, num_updates=60000, accuracy=38.4, loss=4.11\nTrain: wpb=1000, num_updates=61000, accuracy=38.5, loss=4.10\nTrain: wpb=1000, num_updates=62000, accuracy=38.5, loss=4.09\nTrain: wpb=1000, num_updates=63000, accuracy=38.6, loss=4.08\nTrain: wpb=1000, num_updates=64000, accuracy=38.7, loss=4.07\nTrain: wpb=1000, num_updates=65000, accuracy=38.8, loss=4.07\nTrain: wpb=1000, num_updates=66000, accuracy=38.8, loss=4.06\nTrain: wpb=1000, num_updates=67000, accuracy=38.9, loss=4.05\nTrain: wpb=1000, num_updates=68000, accuracy=39.0, loss=4.04\nTrain: wpb=1000, num_updates=69000, accuracy=39.0, loss=4.03\nTrain: wpb=1000, num_updates=70000, accuracy=39.1, loss=4.03\nTrain: wpb=1000, num_updates=71000, accuracy=39.1, loss=4.02\nTrain: wpb=1000, num_updates=72000, accuracy=39.2, loss=4.01\nTrain: wpb=1000, num_updates=73000, accuracy=39.2, loss=4.01\nTrain: wpb=1000, num_updates=74000, accuracy=39.3, loss=4.00\nTrain: wpb=1000, num_updates=75000, accuracy=39.3, loss=3.99\nTrain: wpb=1000, num_updates=76000, accuracy=39.4, loss=3.99\nTrain: wpb=1000, num_updates=77000, accuracy=39.4, loss=3.98\nTrain: wpb=1000, num_updates=78000, accuracy=39.5, loss=3.97\nTrain: wpb=1000, num_updates=79000, accuracy=39.5, loss=3.97\nTrain: wpb=1000, num_updates=80000, accuracy=39.6, loss=3.96\nTrain: wpb=1000, num_updates=81000, accuracy=39.6, loss=3.96\nTrain: wpb=1000, num_updates=82000, accuracy=39.7, loss=3.95\nTrain: wpb=999, num_updates=82285, accuracy=39.7, loss=3.95\n| epoch 000 | train accuracy=39.7%, train loss=3.95\n| epoch 000 | valid accuracy=41.2%, valid loss=3.68 (wikipedia)\n| epoch 000 | valid accuracy=30.3%, valid loss=4.63 (El Periódico)\nTrain: wpb=1000, num_updates=200, accuracy=44.0, loss=3.37\nTrain: wpb=1000, num_updates=500, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=1000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=2000, accuracy=44.0, loss=3.37\nTrain: wpb=1000, num_updates=3000, accuracy=44.0, loss=3.38\nTrain: wpb=1000, num_updates=4000, accuracy=43.9, loss=3.39\nTrain: wpb=1000, num_updates=5000, accuracy=43.9, loss=3.39\nTrain: wpb=1000, num_updates=6000, accuracy=43.9, loss=3.40\nTrain: wpb=1000, num_updates=7000, accuracy=43.8, loss=3.40\nTrain: wpb=1000, num_updates=8000, accuracy=43.8, loss=3.41\nTrain: wpb=1000, num_updates=9000, accuracy=43.8, loss=3.41\nTrain: wpb=1000, num_updates=10000, accuracy=43.7, loss=3.42\nTrain: wpb=1000, num_updates=11000, accuracy=43.7, loss=3.42\nTrain: wpb=1000, num_updates=12000, accuracy=43.7, loss=3.42\nTrain: wpb=1000, num_updates=13000, accuracy=43.6, loss=3.43\nTrain: wpb=1000, num_updates=14000, accuracy=43.6, loss=3.43\nTrain: wpb=1000, num_updates=15000, accuracy=43.6, loss=3.43\nTrain: wpb=1000, num_updates=16000, accuracy=43.6, loss=3.44\nTrain: wpb=1000, num_updates=17000, accuracy=43.5, loss=3.44\nTrain: wpb=1000, num_updates=18000, accuracy=43.5, loss=3.44\nTrain: wpb=1000, num_updates=19000, accuracy=43.5, loss=3.45\nTrain: wpb=1000, num_updates=20000, accuracy=43.5, loss=3.45\nTrain: wpb=1000, num_updates=21000, accuracy=43.5, loss=3.45\nTrain: wpb=1000, num_updates=22000, accuracy=43.4, loss=3.45\nTrain: wpb=1000, num_updates=23000, accuracy=43.4, loss=3.46\nTrain: wpb=1000, num_updates=24000, accuracy=43.4, loss=3.46\nTrain: wpb=1000, num_updates=25000, accuracy=43.4, loss=3.46\nTrain: wpb=1000, num_updates=26000, accuracy=43.4, loss=3.46\nTrain: wpb=1000, num_updates=27000, accuracy=43.3, loss=3.47\nTrain: wpb=1000, num_updates=28000, accuracy=43.3, loss=3.47\nTrain: wpb=1000, num_updates=29000, accuracy=43.3, loss=3.47\nTrain: wpb=1000, num_updates=30000, accuracy=43.3, loss=3.47\nTrain: wpb=1000, num_updates=31000, accuracy=43.3, loss=3.47\nTrain: wpb=1000, num_updates=32000, accuracy=43.3, loss=3.48\nTrain: wpb=1000, num_updates=33000, accuracy=43.3, loss=3.48\nTrain: wpb=1000, num_updates=34000, accuracy=43.2, loss=3.48\nTrain: wpb=1000, num_updates=35000, accuracy=43.2, loss=3.48\nTrain: wpb=1000, num_updates=36000, accuracy=43.2, loss=3.48\nTrain: wpb=1000, num_updates=37000, accuracy=43.2, loss=3.48\nTrain: wpb=1000, num_updates=38000, accuracy=43.2, loss=3.49\nTrain: wpb=1000, num_updates=39000, accuracy=43.2, loss=3.49\nTrain: wpb=1000, num_updates=40000, accuracy=43.2, loss=3.49\nTrain: wpb=1000, num_updates=41000, accuracy=43.2, loss=3.49\nTrain: wpb=1000, num_updates=42000, accuracy=43.2, loss=3.49\nTrain: wpb=1000, num_updates=43000, accuracy=43.1, loss=3.49\nTrain: wpb=1000, num_updates=44000, accuracy=43.1, loss=3.49\nTrain: wpb=1000, num_updates=45000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=46000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=47000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=48000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=49000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=50000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=51000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=52000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=53000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=54000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=55000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=56000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=57000, accuracy=43.1, loss=3.50\nTrain: wpb=1000, num_updates=58000, accuracy=43.1, loss=3.51\nTrain: wpb=1000, num_updates=59000, accuracy=43.1, loss=3.51\nTrain: wpb=1000, num_updates=60000, accuracy=43.1, loss=3.51\nTrain: wpb=1000, num_updates=61000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=62000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=63000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=64000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=65000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=66000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=67000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=68000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=69000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=70000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=71000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=72000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=73000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=74000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=75000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=76000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=77000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=78000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=79000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=80000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=81000, accuracy=43.0, loss=3.51\nTrain: wpb=1000, num_updates=82000, accuracy=43.0, loss=3.51\nTrain: wpb=999, num_updates=82285, accuracy=43.0, loss=3.51\n| epoch 001 | train accuracy=43.0%, train loss=3.51\n| epoch 001 | valid accuracy=40.8%, valid loss=3.72 (wikipedia)\n| epoch 001 | valid accuracy=30.3%, valid loss=4.68 (El Periódico)\nTrain: wpb=1000, num_updates=200, accuracy=44.5, loss=3.29\nTrain: wpb=1000, num_updates=500, accuracy=44.5, loss=3.29\nTrain: wpb=1000, num_updates=1000, accuracy=44.5, loss=3.30\nTrain: wpb=1000, num_updates=2000, accuracy=44.4, loss=3.31\nTrain: wpb=1000, num_updates=3000, accuracy=44.4, loss=3.31\nTrain: wpb=1000, num_updates=4000, accuracy=44.3, loss=3.32\nTrain: wpb=1000, num_updates=5000, accuracy=44.3, loss=3.32\nTrain: wpb=1000, num_updates=6000, accuracy=44.3, loss=3.33\nTrain: wpb=1000, num_updates=7000, accuracy=44.3, loss=3.33\nTrain: wpb=1000, num_updates=8000, accuracy=44.2, loss=3.33\nTrain: wpb=1000, num_updates=9000, accuracy=44.2, loss=3.34\nTrain: wpb=1000, num_updates=10000, accuracy=44.2, loss=3.34\nTrain: wpb=1000, num_updates=11000, accuracy=44.2, loss=3.34\nTrain: wpb=1000, num_updates=12000, accuracy=44.2, loss=3.34\nTrain: wpb=1000, num_updates=13000, accuracy=44.2, loss=3.34\nTrain: wpb=1000, num_updates=14000, accuracy=44.1, loss=3.35\nTrain: wpb=1000, num_updates=15000, accuracy=44.1, loss=3.35\nTrain: wpb=1000, num_updates=16000, accuracy=44.1, loss=3.35\nTrain: wpb=1000, num_updates=17000, accuracy=44.1, loss=3.35\nTrain: wpb=1000, num_updates=18000, accuracy=44.1, loss=3.35\nTrain: wpb=1000, num_updates=19000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=20000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=21000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=22000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=23000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=24000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=25000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=26000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=27000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=28000, accuracy=44.1, loss=3.36\nTrain: wpb=1000, num_updates=29000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=30000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=31000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=32000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=33000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=34000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=35000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=36000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=37000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=38000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=39000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=40000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=41000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=42000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=43000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=44000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=45000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=46000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=47000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=48000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=49000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=50000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=51000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=52000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=53000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=54000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=55000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=56000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=57000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=58000, accuracy=44.1, loss=3.37\nTrain: wpb=1000, num_updates=59000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=60000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=61000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=62000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=63000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=64000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=65000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=66000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=67000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=68000, accuracy=44.2, loss=3.36\nTrain: wpb=1000, num_updates=69000, accuracy=44.2, loss=3.36\n","output_type":"stream"}]},{"cell_type":"code","source":"# Test prediction\ny_pred = validate(model, None, test_x, None, params.batch_size, device)\ny_token = [vocab.idx2token[index] for index in y_pred]","metadata":{"papermill":{"duration":0.198128,"end_time":"2022-03-07T17:32:42.012133","exception":false,"start_time":"2022-03-07T17:32:41.814005","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame({'id':valid_x_df['id'], 'token': y_token}, columns=['id', 'token'])\nprint(submission.head())\nsubmission.to_csv('submission.csv', index=False)","metadata":{"papermill":{"duration":0.15479,"end_time":"2022-03-07T17:32:42.271937","exception":false,"start_time":"2022-03-07T17:32:42.117147","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}